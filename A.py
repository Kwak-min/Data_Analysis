"""
ê°€ì „ì œí’ˆ íŒë§¤ ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„ í”„ë¡œê·¸ë¨
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸: ARIMA, Prophet, Random Forest
- ë”¥ëŸ¬ë‹ ëª¨ë¸: LSTM, GRU
- ì„±ëŠ¥ ë¹„êµ ë° ì‹œê°í™”
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
import warnings
warnings.filterwarnings('ignore')

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.rcParams['font.family'] = 'Malgun Gothic'
plt.rcParams['axes.unicode_minus'] = False

# ë¨¸ì‹ ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, mean_absolute_percentage_error

# ì‹œê³„ì—´ ë¶„ì„ ë¼ì´ë¸ŒëŸ¬ë¦¬
from statsmodels.tsa.arima.model import ARIMA
from statsmodels.tsa.statespace.sarimax import SARIMAX

# ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import LSTM, GRU, Dense, Dropout
    from tensorflow.keras.callbacks import EarlyStopping
    DEEP_LEARNING_AVAILABLE = True
except ImportError:
    print("ê²½ê³ : TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install tensorflow")
    DEEP_LEARNING_AVAILABLE = False

# Prophet ë¼ì´ë¸ŒëŸ¬ë¦¬
try:
    from prophet import Prophet
    PROPHET_AVAILABLE = True
except ImportError:
    print("ê²½ê³ : Prophetì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. Prophet ëª¨ë¸ì€ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    print("ì„¤ì¹˜ ë°©ë²•: pip install prophet")
    PROPHET_AVAILABLE = False


class ApplianceSalesPredictor:
    """ê°€ì „ì œí’ˆ íŒë§¤ ì˜ˆì¸¡ ì‹œìŠ¤í…œ"""

    def __init__(self, csv_path):
        """
        ì´ˆê¸°í™” í•¨ìˆ˜

        Parameters:
        -----------
        csv_path : str
            ë°ì´í„° CSV íŒŒì¼ ê²½ë¡œ
        """
        self.csv_path = csv_path
        self.df = None
        self.daily_sales = None
        self.scaler = MinMaxScaler()
        self.results = {}

    def load_and_prepare_data(self):
        """ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        print("="*80)
        print("[1ë‹¨ê³„] ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬")
        print("="*80)

        # ë°ì´í„° ë¡œë“œ
        self.df = pd.read_csv(self.csv_path)
        print(f"\nì›ë³¸ ë°ì´í„° í¬ê¸°: {self.df.shape}")
        print(f"ì»¬ëŸ¼: {list(self.df.columns)}")

        # ë‚ ì§œ í˜•ì‹ ë³€í™˜
        self.df['ë‚ ì§œ'] = pd.to_datetime(self.df['ë‚ ì§œ'], format='%Y%m%d')

        # ì¼ë³„ ì´ ë§¤ì¶œ ì§‘ê³„
        self.daily_sales = self.df.groupby('ë‚ ì§œ')['ë§¤ì¶œê¸ˆì•¡'].sum().reset_index()
        self.daily_sales.columns = ['ë‚ ì§œ', 'ë§¤ì¶œê¸ˆì•¡']
        self.daily_sales = self.daily_sales.sort_values('ë‚ ì§œ').reset_index(drop=True)

        print(f"\nì¼ë³„ ì§‘ê³„ ë°ì´í„° í¬ê¸°: {self.daily_sales.shape}")
        print(f"ê¸°ê°„: {self.daily_sales['ë‚ ì§œ'].min()} ~ {self.daily_sales['ë‚ ì§œ'].max()}")
        print(f"í‰ê·  ì¼ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].mean():,.0f}ì›")
        print(f"ìµœëŒ€ ì¼ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].max():,.0f}ì›")
        print(f"ìµœì†Œ ì¼ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].min():,.0f}ì›")

        # ì¶”ê°€ í”¼ì²˜ ìƒì„±
        self.daily_sales['ìš”ì¼'] = self.daily_sales['ë‚ ì§œ'].dt.dayofweek
        self.daily_sales['ì¼'] = self.daily_sales['ë‚ ì§œ'].dt.day
        self.daily_sales['ì£¼ë§ì—¬ë¶€'] = (self.daily_sales['ìš”ì¼'] >= 5).astype(int)

        return self.daily_sales

    def visualize_data(self):
        """ë°ì´í„° ì‹œê°í™”"""
        print("\n[ë°ì´í„° ì‹œê°í™”]")

        fig, axes = plt.subplots(2, 2, figsize=(16, 10))

        # 1. ì¼ë³„ ë§¤ì¶œ ì¶”ì´
        axes[0, 0].plot(self.daily_sales['ë‚ ì§œ'], self.daily_sales['ë§¤ì¶œê¸ˆì•¡'],
                       marker='o', linewidth=2, markersize=6, color='#2E86AB')
        axes[0, 0].set_title('ê°€ì „ì œí’ˆ ì¼ë³„ ë§¤ì¶œ ì¶”ì´', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('ë‚ ì§œ', fontsize=11)
        axes[0, 0].set_ylabel('ë§¤ì¶œê¸ˆì•¡ (ì›)', fontsize=11)
        axes[0, 0].grid(True, alpha=0.3)
        axes[0, 0].tick_params(axis='x', rotation=45)

        # 2. ìš”ì¼ë³„ í‰ê·  ë§¤ì¶œ
        dow_sales = self.daily_sales.groupby('ìš”ì¼')['ë§¤ì¶œê¸ˆì•¡'].mean()
        dow_names = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼']
        axes[0, 1].bar(range(7), dow_sales.values, color='#A23B72')
        axes[0, 1].set_title('ìš”ì¼ë³„ í‰ê·  ë§¤ì¶œ', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('ìš”ì¼', fontsize=11)
        axes[0, 1].set_ylabel('í‰ê·  ë§¤ì¶œê¸ˆì•¡ (ì›)', fontsize=11)
        axes[0, 1].set_xticks(range(7))
        axes[0, 1].set_xticklabels(dow_names)
        axes[0, 1].grid(True, alpha=0.3, axis='y')

        # 3. ë§¤ì¶œ ë¶„í¬
        axes[1, 0].hist(self.daily_sales['ë§¤ì¶œê¸ˆì•¡'], bins=15, color='#F18F01', edgecolor='black')
        axes[1, 0].set_title('ë§¤ì¶œê¸ˆì•¡ ë¶„í¬', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('ë§¤ì¶œê¸ˆì•¡ (ì›)', fontsize=11)
        axes[1, 0].set_ylabel('ë¹ˆë„', fontsize=11)
        axes[1, 0].grid(True, alpha=0.3, axis='y')

        # 4. ì£¼ë§ vs í‰ì¼ ë°•ìŠ¤í”Œë¡¯
        weekend_data = [
            self.daily_sales[self.daily_sales['ì£¼ë§ì—¬ë¶€'] == 0]['ë§¤ì¶œê¸ˆì•¡'].values,
            self.daily_sales[self.daily_sales['ì£¼ë§ì—¬ë¶€'] == 1]['ë§¤ì¶œê¸ˆì•¡'].values
        ]
        bp = axes[1, 1].boxplot(weekend_data, labels=['í‰ì¼', 'ì£¼ë§'], patch_artist=True)
        for patch, color in zip(bp['boxes'], ['#6A994E', '#BC4749']):
            patch.set_facecolor(color)
        axes[1, 1].set_title('í‰ì¼ vs ì£¼ë§ ë§¤ì¶œ ë¹„êµ', fontsize=14, fontweight='bold')
        axes[1, 1].set_ylabel('ë§¤ì¶œê¸ˆì•¡ (ì›)', fontsize=11)
        axes[1, 1].grid(True, alpha=0.3, axis='y')

        plt.tight_layout()
        plt.savefig('01_ë°ì´í„°_ì‹œê°í™”.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("âœ“ ì‹œê°í™” ì™„ë£Œ: 01_ë°ì´í„°_ì‹œê°í™”.png ì €ì¥ë¨")

    def split_data(self, train_ratio=0.8):
        """í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¶„ë¦¬"""
        split_idx = int(len(self.daily_sales) * train_ratio)

        self.train_data = self.daily_sales[:split_idx].copy()
        self.test_data = self.daily_sales[split_idx:].copy()

        print(f"\n[ë°ì´í„° ë¶„ë¦¬]")
        print(f"í•™ìŠµ ë°ì´í„°: {len(self.train_data)}ì¼ ({self.train_data['ë‚ ì§œ'].min()} ~ {self.train_data['ë‚ ì§œ'].max()})")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.test_data)}ì¼ ({self.test_data['ë‚ ì§œ'].min()} ~ {self.test_data['ë‚ ì§œ'].max()})")

        return self.train_data, self.test_data

    def calculate_metrics(self, y_true, y_pred, model_name):
        """ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°"""
        mse = mean_squared_error(y_true, y_pred)
        rmse = np.sqrt(mse)
        mae = mean_absolute_error(y_true, y_pred)
        r2 = r2_score(y_true, y_pred)

        # MAPE ê³„ì‚° (0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€)
        mask = y_true != 0
        mape = np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100 if mask.any() else 0

        metrics = {
            'MSE': mse,
            'RMSE': rmse,
            'MAE': mae,
            'R2': r2,
            'MAPE': mape
        }

        self.results[model_name] = {
            'predictions': y_pred,
            'metrics': metrics
        }

        return metrics

    def print_metrics(self, metrics, model_name):
        """ì„±ëŠ¥ ì§€í‘œ ì¶œë ¥"""
        print(f"\n{'='*60}")
        print(f"[{model_name} ì„±ëŠ¥ ì§€í‘œ]")
        print(f"{'='*60}")
        print(f"MSE (Mean Squared Error)      : {metrics['MSE']:,.0f}")
        print(f"RMSE (Root Mean Squared Error): {metrics['RMSE']:,.0f}ì›")
        print(f"MAE (Mean Absolute Error)     : {metrics['MAE']:,.0f}ì›")
        print(f"RÂ² Score                      : {metrics['R2']:.4f}")
        print(f"MAPE (Mean Absolute % Error)  : {metrics['MAPE']:.2f}%")
        print(f"{'='*60}")

    # ========================================================================
    # ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ë“¤
    # ========================================================================

    def train_arima(self, order=(2, 1, 2)):
        """ARIMA ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        print("\n" + "="*80)
        print("[ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ 1] ARIMA ëª¨ë¸")
        print("="*80)

        try:
            # ARIMA ëª¨ë¸ í•™ìŠµ
            model = ARIMA(self.train_data['ë§¤ì¶œê¸ˆì•¡'], order=order)
            fitted_model = model.fit()

            # ì˜ˆì¸¡
            forecast_steps = len(self.test_data)
            predictions = fitted_model.forecast(steps=forecast_steps)

            # ì„±ëŠ¥ í‰ê°€
            y_true = self.test_data['ë§¤ì¶œê¸ˆì•¡'].values
            metrics = self.calculate_metrics(y_true, predictions, 'ARIMA')
            self.print_metrics(metrics, f'ARIMA{order}')

            return predictions, metrics

        except Exception as e:
            print(f"ARIMA ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None

    def train_prophet(self):
        """Prophet ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not PROPHET_AVAILABLE:
            print("\nProphet ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        print("\n" + "="*80)
        print("[ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ 2] Prophet ëª¨ë¸")
        print("="*80)

        try:
            # Prophet í˜•ì‹ìœ¼ë¡œ ë°ì´í„° ë³€í™˜
            train_prophet = self.train_data[['ë‚ ì§œ', 'ë§¤ì¶œê¸ˆì•¡']].copy()
            train_prophet.columns = ['ds', 'y']

            # ëª¨ë¸ í•™ìŠµ
            model = Prophet(
                daily_seasonality=True,
                weekly_seasonality=True,
                yearly_seasonality=False,
                changepoint_prior_scale=0.05
            )
            model.fit(train_prophet)

            # ë¯¸ë˜ ë‚ ì§œ ìƒì„± ë° ì˜ˆì¸¡
            future = model.make_future_dataframe(periods=len(self.test_data))
            forecast = model.predict(future)

            # í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì˜ˆì¸¡ê°’ ì¶”ì¶œ
            predictions = forecast['yhat'].iloc[-len(self.test_data):].values

            # ì„±ëŠ¥ í‰ê°€
            y_true = self.test_data['ë§¤ì¶œê¸ˆì•¡'].values
            metrics = self.calculate_metrics(y_true, predictions, 'Prophet')
            self.print_metrics(metrics, 'Prophet')

            return predictions, metrics

        except Exception as e:
            print(f"Prophet ëª¨ë¸ í•™ìŠµ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None, None

    def train_random_forest(self):
        """Random Forest ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        print("\n" + "="*80)
        print("[ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ 3] Random Forest ëª¨ë¸")
        print("="*80)

        # í”¼ì²˜ ì¤€ë¹„
        features = ['ìš”ì¼', 'ì¼', 'ì£¼ë§ì—¬ë¶€']

        X_train = self.train_data[features].values
        y_train = self.train_data['ë§¤ì¶œê¸ˆì•¡'].values
        X_test = self.test_data[features].values
        y_test = self.test_data['ë§¤ì¶œê¸ˆì•¡'].values

        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestRegressor(
            n_estimators=200,
            max_depth=10,
            min_samples_split=5,
            min_samples_leaf=2,
            random_state=42,
            n_jobs=-1
        )
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡
        predictions = model.predict(X_test)

        # í”¼ì²˜ ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': features,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)

        print("\n[í”¼ì²˜ ì¤‘ìš”ë„]")
        print(feature_importance)

        # ì„±ëŠ¥ í‰ê°€
        metrics = self.calculate_metrics(y_test, predictions, 'RandomForest')
        self.print_metrics(metrics, 'Random Forest')

        return predictions, metrics

    def train_gradient_boosting(self):
        """Gradient Boosting ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        print("\n" + "="*80)
        print("[ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ 4] Gradient Boosting ëª¨ë¸")
        print("="*80)

        # í”¼ì²˜ ì¤€ë¹„
        features = ['ìš”ì¼', 'ì¼', 'ì£¼ë§ì—¬ë¶€']

        X_train = self.train_data[features].values
        y_train = self.train_data['ë§¤ì¶œê¸ˆì•¡'].values
        X_test = self.test_data[features].values
        y_test = self.test_data['ë§¤ì¶œê¸ˆì•¡'].values

        # ëª¨ë¸ í•™ìŠµ
        model = GradientBoostingRegressor(
            n_estimators=200,
            learning_rate=0.1,
            max_depth=5,
            min_samples_split=5,
            random_state=42
        )
        model.fit(X_train, y_train)

        # ì˜ˆì¸¡
        predictions = model.predict(X_test)

        # ì„±ëŠ¥ í‰ê°€
        metrics = self.calculate_metrics(y_test, predictions, 'GradientBoosting')
        self.print_metrics(metrics, 'Gradient Boosting')

        return predictions, metrics

    # ========================================================================
    # ë”¥ëŸ¬ë‹ ëª¨ë¸ë“¤
    # ========================================================================

    def create_sequences(self, data, seq_length):
        """ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ë”¥ëŸ¬ë‹ìš©)"""
        X, y = [], []
        for i in range(len(data) - seq_length):
            X.append(data[i:i+seq_length])
            y.append(data[i+seq_length])
        return np.array(X), np.array(y)

    def train_lstm(self, seq_length=7, epochs=100, batch_size=8):
        """LSTM ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not DEEP_LEARNING_AVAILABLE:
            print("\në”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        print("\n" + "="*80)
        print("[ë”¥ëŸ¬ë‹ ëª¨ë¸ 1] LSTM ëª¨ë¸")
        print("="*80)

        # ë°ì´í„° ì •ê·œí™”
        sales_data = self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].values.reshape(-1, 1)
        scaled_data = self.scaler.fit_transform(sales_data)

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self.create_sequences(scaled_data, seq_length)

        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
        split_idx = len(self.train_data) - seq_length
        X_train, y_train = X[:split_idx], y[:split_idx]
        X_test, y_test = X[split_idx:], y[split_idx:]

        print(f"\ní•™ìŠµ ë°ì´í„° shape: {X_train.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {X_test.shape}")

        # LSTM ëª¨ë¸ êµ¬ì¶•
        model = Sequential([
            LSTM(64, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),
            Dropout(0.2),
            LSTM(32, activation='relu'),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1)
        ])

        model.compile(optimizer='adam', loss='mse', metrics=['mae'])

        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        early_stop = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)

        # ëª¨ë¸ í•™ìŠµ
        print("\n[ëª¨ë¸ í•™ìŠµ ì¤‘...]")
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stop],
            verbose=0
        )

        print(f"âœ“ í•™ìŠµ ì™„ë£Œ (Epochs: {len(history.history['loss'])})")

        # ì˜ˆì¸¡
        predictions_scaled = model.predict(X_test, verbose=0)
        predictions = self.scaler.inverse_transform(predictions_scaled).flatten()
        y_test_original = self.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

        # ì„±ëŠ¥ í‰ê°€
        metrics = self.calculate_metrics(y_test_original, predictions, 'LSTM')
        self.print_metrics(metrics, f'LSTM (seq_length={seq_length})')

        # ê²°ê³¼ ì €ì¥
        self.results['LSTM']['history'] = history.history

        return predictions, metrics

    def train_gru(self, seq_length=7, epochs=100, batch_size=8):
        """GRU ëª¨ë¸ í•™ìŠµ ë° ì˜ˆì¸¡"""
        if not DEEP_LEARNING_AVAILABLE:
            print("\në”¥ëŸ¬ë‹ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return None, None

        print("\n" + "="*80)
        print("[ë”¥ëŸ¬ë‹ ëª¨ë¸ 2] GRU ëª¨ë¸")
        print("="*80)

        # ë°ì´í„° ì •ê·œí™”
        sales_data = self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].values.reshape(-1, 1)
        scaled_data = self.scaler.fit_transform(sales_data)

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = self.create_sequences(scaled_data, seq_length)

        # í•™ìŠµ/í…ŒìŠ¤íŠ¸ ë¶„ë¦¬
        split_idx = len(self.train_data) - seq_length
        X_train, y_train = X[:split_idx], y[:split_idx]
        X_test, y_test = X[split_idx:], y[split_idx:]

        print(f"\ní•™ìŠµ ë°ì´í„° shape: {X_train.shape}")
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° shape: {X_test.shape}")

        # GRU ëª¨ë¸ êµ¬ì¶•
        model = Sequential([
            GRU(64, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),
            Dropout(0.2),
            GRU(32, activation='relu'),
            Dropout(0.2),
            Dense(16, activation='relu'),
            Dense(1)
        ])

        model.compile(optimizer='adam', loss='mse', metrics=['mae'])

        # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
        early_stop = EarlyStopping(monitor='loss', patience=15, restore_best_weights=True)

        # ëª¨ë¸ í•™ìŠµ
        print("\n[ëª¨ë¸ í•™ìŠµ ì¤‘...]")
        history = model.fit(
            X_train, y_train,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=[early_stop],
            verbose=0
        )

        print(f"âœ“ í•™ìŠµ ì™„ë£Œ (Epochs: {len(history.history['loss'])})")

        # ì˜ˆì¸¡
        predictions_scaled = model.predict(X_test, verbose=0)
        predictions = self.scaler.inverse_transform(predictions_scaled).flatten()
        y_test_original = self.scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()

        # ì„±ëŠ¥ í‰ê°€
        metrics = self.calculate_metrics(y_test_original, predictions, 'GRU')
        self.print_metrics(metrics, f'GRU (seq_length={seq_length})')

        # ê²°ê³¼ ì €ì¥
        self.results['GRU']['history'] = history.history

        return predictions, metrics

    # ========================================================================
    # ê²°ê³¼ ë¹„êµ ë° ì‹œê°í™”
    # ========================================================================

    def compare_all_models(self):
        """ëª¨ë“  ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ"""
        print("\n" + "="*80)
        print("[ìµœì¢… ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ]")
        print("="*80)

        # ê²°ê³¼ í…Œì´ë¸” ìƒì„±
        comparison_data = []
        for model_name, result in self.results.items():
            metrics = result['metrics']
            comparison_data.append({
                'ëª¨ë¸': model_name,
                'RMSE': metrics['RMSE'],
                'MAE': metrics['MAE'],
                'RÂ²': metrics['R2'],
                'MAPE(%)': metrics['MAPE']
            })

        comparison_df = pd.DataFrame(comparison_data)
        comparison_df = comparison_df.sort_values('RMSE')

        print("\n")
        print(comparison_df.to_string(index=False))

        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸
        best_model = comparison_df.iloc[0]['ëª¨ë¸']
        print(f"\n{'='*80}")
        print(f"ğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model}")
        print(f"{'='*80}")

        return comparison_df

    def visualize_predictions(self):
        """ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”"""
        print("\n[ì˜ˆì¸¡ ê²°ê³¼ ì‹œê°í™”]")

        n_models = len(self.results)
        fig, axes = plt.subplots(n_models, 1, figsize=(16, 5*n_models))

        if n_models == 1:
            axes = [axes]

        y_true = self.test_data['ë§¤ì¶œê¸ˆì•¡'].values
        test_dates = self.test_data['ë‚ ì§œ'].values

        for idx, (model_name, result) in enumerate(self.results.items()):
            y_pred = result['predictions']

            # ê¸¸ì´ ë§ì¶”ê¸° (ë”¥ëŸ¬ë‹ ëª¨ë¸ì˜ ê²½ìš°)
            if len(y_pred) < len(y_true):
                y_true_plot = y_true[-len(y_pred):]
                test_dates_plot = test_dates[-len(y_pred):]
            else:
                y_true_plot = y_true
                test_dates_plot = test_dates

            axes[idx].plot(test_dates_plot, y_true_plot,
                          marker='o', linewidth=2, markersize=8,
                          label='ì‹¤ì œ ë§¤ì¶œ', color='#2E86AB')
            axes[idx].plot(test_dates_plot, y_pred[:len(y_true_plot)],
                          marker='s', linewidth=2, markersize=8,
                          label='ì˜ˆì¸¡ ë§¤ì¶œ', color='#F18F01', linestyle='--')

            axes[idx].set_title(f'{model_name} ëª¨ë¸ - ì‹¤ì œ vs ì˜ˆì¸¡',
                               fontsize=14, fontweight='bold')
            axes[idx].set_xlabel('ë‚ ì§œ', fontsize=11)
            axes[idx].set_ylabel('ë§¤ì¶œê¸ˆì•¡ (ì›)', fontsize=11)
            axes[idx].legend(fontsize=11)
            axes[idx].grid(True, alpha=0.3)
            axes[idx].tick_params(axis='x', rotation=45)

            # RÂ² ì ìˆ˜ í‘œì‹œ
            r2 = result['metrics']['R2']
            mae = result['metrics']['MAE']
            axes[idx].text(0.02, 0.98, f"RÂ² = {r2:.4f}\nMAE = {mae:,.0f}ì›",
                          transform=axes[idx].transAxes,
                          verticalalignment='top',
                          bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5),
                          fontsize=10)

        plt.tight_layout()
        plt.savefig('02_ëª¨ë¸_ì˜ˆì¸¡_ë¹„êµ.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("âœ“ ì‹œê°í™” ì™„ë£Œ: 02_ëª¨ë¸_ì˜ˆì¸¡_ë¹„êµ.png ì €ì¥ë¨")

    def visualize_metrics_comparison(self, comparison_df):
        """ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì‹œê°í™”"""
        print("\n[ì„±ëŠ¥ ì§€í‘œ ë¹„êµ ì‹œê°í™”]")

        fig, axes = plt.subplots(2, 2, figsize=(16, 10))

        models = comparison_df['ëª¨ë¸'].values

        # 1. RMSE ë¹„êµ
        axes[0, 0].barh(models, comparison_df['RMSE'].values, color='#E63946')
        axes[0, 0].set_title('RMSE ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, fontweight='bold')
        axes[0, 0].set_xlabel('RMSE', fontsize=11)
        axes[0, 0].grid(True, alpha=0.3, axis='x')

        # 2. MAE ë¹„êµ
        axes[0, 1].barh(models, comparison_df['MAE'].values, color='#F18F01')
        axes[0, 1].set_title('MAE ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, fontweight='bold')
        axes[0, 1].set_xlabel('MAE', fontsize=11)
        axes[0, 1].grid(True, alpha=0.3, axis='x')

        # 3. RÂ² ë¹„êµ
        axes[1, 0].barh(models, comparison_df['RÂ²'].values, color='#2A9D8F')
        axes[1, 0].set_title('RÂ² Score ë¹„êµ (ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, fontweight='bold')
        axes[1, 0].set_xlabel('RÂ² Score', fontsize=11)
        axes[1, 0].grid(True, alpha=0.3, axis='x')
        axes[1, 0].set_xlim([0, 1])

        # 4. MAPE ë¹„êµ
        axes[1, 1].barh(models, comparison_df['MAPE(%)'].values, color='#A23B72')
        axes[1, 1].set_title('MAPE ë¹„êµ (ë‚®ì„ìˆ˜ë¡ ì¢‹ìŒ)', fontsize=14, fontweight='bold')
        axes[1, 1].set_xlabel('MAPE (%)', fontsize=11)
        axes[1, 1].grid(True, alpha=0.3, axis='x')

        plt.tight_layout()
        plt.savefig('03_ì„±ëŠ¥_ì§€í‘œ_ë¹„êµ.png', dpi=300, bbox_inches='tight')
        plt.show()

        print("âœ“ ì‹œê°í™” ì™„ë£Œ: 03_ì„±ëŠ¥_ì§€í‘œ_ë¹„êµ.png ì €ì¥ë¨")

    def generate_report(self, comparison_df):
        """ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸ ìƒì„±"""
        print("\n" + "="*80)
        print("[ìµœì¢… ë¶„ì„ ë¦¬í¬íŠ¸]")
        print("="*80)

        report = []
        report.append("="*80)
        report.append("ê°€ì „ì œí’ˆ íŒë§¤ ì‹œê³„ì—´ ì˜ˆì¸¡ ë¶„ì„ ë¦¬í¬íŠ¸")
        report.append("="*80)
        report.append(f"\në¶„ì„ ê¸°ê°„: {self.daily_sales['ë‚ ì§œ'].min()} ~ {self.daily_sales['ë‚ ì§œ'].max()}")
        report.append(f"ì´ ë°ì´í„°: {len(self.daily_sales)}ì¼")
        report.append(f"í•™ìŠµ ë°ì´í„°: {len(self.train_data)}ì¼")
        report.append(f"í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(self.test_data)}ì¼")

        report.append("\n" + "-"*80)
        report.append("1. ë°ì´í„° ê¸°ì´ˆ í†µê³„")
        report.append("-"*80)
        report.append(f"í‰ê·  ì¼ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].mean():,.0f}ì›")
        report.append(f"í‘œì¤€í¸ì°¨: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].std():,.0f}ì›")
        report.append(f"ìµœëŒ€ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].max():,.0f}ì›")
        report.append(f"ìµœì†Œ ë§¤ì¶œ: {self.daily_sales['ë§¤ì¶œê¸ˆì•¡'].min():,.0f}ì›")

        report.append("\n" + "-"*80)
        report.append("2. ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ")
        report.append("-"*80)
        report.append("\n" + comparison_df.to_string(index=False))

        report.append("\n" + "-"*80)
        report.append("3. ê²°ë¡  ë° ì œì•ˆ")
        report.append("-"*80)

        best_model = comparison_df.iloc[0]
        worst_model = comparison_df.iloc[-1]

        report.append(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model['ëª¨ë¸']}")
        report.append(f"   - RMSE: {best_model['RMSE']:,.0f}ì›")
        report.append(f"   - MAE: {best_model['MAE']:,.0f}ì›")
        report.append(f"   - RÂ²: {best_model['RÂ²']:.4f}")
        report.append(f"   - MAPE: {best_model['MAPE(%)']:.2f}%")

        report.append(f"\nğŸ“Š ìµœì € ì„±ëŠ¥ ëª¨ë¸: {worst_model['ëª¨ë¸']}")
        report.append(f"   - RMSE: {worst_model['RMSE']:,.0f}ì›")
        report.append(f"   - MAE: {worst_model['MAE']:,.0f}ì›")
        report.append(f"   - RÂ²: {worst_model['RÂ²']:.4f}")
        report.append(f"   - MAPE: {worst_model['MAPE(%)']:.2f}%")

        # ëª¨ë¸ë³„ íŠ¹ì§• ë¶„ì„
        report.append("\n" + "-"*80)
        report.append("4. ëª¨ë¸ë³„ íŠ¹ì§• ë° ë¶„ì„")
        report.append("-"*80)

        if 'ARIMA' in self.results:
            report.append("\n[ARIMA]")
            report.append("- ì „í†µì ì¸ ì‹œê³„ì—´ ë¶„ì„ ê¸°ë²•")
            report.append("- ì„ í˜• ì¶”ì„¸ì™€ ê³„ì ˆì„± íŒ¨í„´ í¬ì°©ì— ê°•ì ")
            report.append("- ë‹¨ê¸° ì˜ˆì¸¡ì— íš¨ê³¼ì ")

        if 'Prophet' in self.results:
            report.append("\n[Prophet]")
            report.append("- Facebook ê°œë°œ ì‹œê³„ì—´ ì˜ˆì¸¡ ë¼ì´ë¸ŒëŸ¬ë¦¬")
            report.append("- ì¶”ì„¸, ê³„ì ˆì„±, íœ´ì¼ íš¨ê³¼ ìë™ ê°ì§€")
            report.append("- ê²°ì¸¡ì¹˜ì™€ ì´ìƒì¹˜ì— ê°•ê±´í•¨")

        if 'RandomForest' in self.results:
            report.append("\n[Random Forest]")
            report.append("- ì•™ìƒë¸” ë¨¸ì‹ ëŸ¬ë‹ ê¸°ë²•")
            report.append("- ë¹„ì„ í˜• íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥")
            report.append("- í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ ê°€ëŠ¥")

        if 'GradientBoosting' in self.results:
            report.append("\n[Gradient Boosting]")
            report.append("- ë¶€ìŠ¤íŒ… ê¸°ë°˜ ì•™ìƒë¸” ê¸°ë²•")
            report.append("- ìˆœì°¨ì  í•™ìŠµìœ¼ë¡œ ì˜¤ì°¨ ìµœì†Œí™”")
            report.append("- ë³µì¡í•œ íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥")

        if 'LSTM' in self.results:
            report.append("\n[LSTM]")
            report.append("- ì¥ë‹¨ê¸° ë©”ëª¨ë¦¬ ì‹ ê²½ë§")
            report.append("- ì¥ê¸° ì˜ì¡´ì„± íŒ¨í„´ í•™ìŠµ ê°€ëŠ¥")
            report.append("- ë³µì¡í•œ ì‹œê³„ì—´ íŒ¨í„´ í¬ì°©")

        if 'GRU' in self.results:
            report.append("\n[GRU]")
            report.append("- LSTMì˜ ê²½ëŸ‰í™” ë²„ì „")
            report.append("- í•™ìŠµ ì†ë„ê°€ ë¹ ë¦„")
            report.append("- LSTMê³¼ ìœ ì‚¬í•œ ì„±ëŠ¥")

        report.append("\n" + "-"*80)
        report.append("5. ê¶Œì¥ì‚¬í•­")
        report.append("-"*80)
        report.append(f"\nâœ“ ì‹¤ë¬´ ì ìš© ê¶Œì¥ ëª¨ë¸: {best_model['ëª¨ë¸']}")
        report.append("âœ“ ì •ê¸°ì ì¸ ëª¨ë¸ ì¬í•™ìŠµ í•„ìš” (ì‹ ê·œ ë°ì´í„° ë°˜ì˜)")
        report.append("âœ“ ì™¸ë¶€ ë³€ìˆ˜(í”„ë¡œëª¨ì…˜, ê³„ì ˆì„± ë“±) ì¶”ê°€ ê³ ë ¤")
        report.append("âœ“ ì•™ìƒë¸” ê¸°ë²•ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ ê²°í•© ê²€í† ")

        report.append("\n" + "="*80)
        report.append(f"ë¦¬í¬íŠ¸ ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append("="*80)

        # ì¶œë ¥ ë° ì €ì¥
        report_text = "\n".join(report)
        print(report_text)

        with open('04_ë¶„ì„_ë¦¬í¬íŠ¸.txt', 'w', encoding='utf-8') as f:
            f.write(report_text)

        print("\nâœ“ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: 04_ë¶„ì„_ë¦¬í¬íŠ¸.txt")

        return report_text

    def run_full_analysis(self):
        """ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        print("\n")
        print("â–ˆ" * 80)
        print("â–ˆ" + " " * 78 + "â–ˆ")
        print("â–ˆ" + " " * 20 + "ê°€ì „ì œí’ˆ íŒë§¤ ì˜ˆì¸¡ ë¶„ì„ ì‹œìŠ¤í…œ" + " " * 20 + "â–ˆ")
        print("â–ˆ" + " " * 78 + "â–ˆ")
        print("â–ˆ" * 80)
        print("\n")

        # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
        self.load_and_prepare_data()
        self.visualize_data()

        # 2. ë°ì´í„° ë¶„ë¦¬
        self.split_data(train_ratio=0.8)

        # 3. ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
        print("\n" + "â–ˆ"*80)
        print("â–ˆ" + " "*25 + "ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘" + " "*25 + "â–ˆ")
        print("â–ˆ"*80)

        self.train_arima()
        self.train_prophet()
        self.train_random_forest()
        self.train_gradient_boosting()

        # 4. ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ
        if DEEP_LEARNING_AVAILABLE:
            print("\n" + "â–ˆ"*80)
            print("â–ˆ" + " "*26 + "ë”¥ëŸ¬ë‹ ëª¨ë¸ í•™ìŠµ ì‹œì‘" + " "*26 + "â–ˆ")
            print("â–ˆ"*80)

            self.train_lstm(seq_length=7, epochs=100)
            self.train_gru(seq_length=7, epochs=100)

        # 5. ê²°ê³¼ ë¹„êµ
        comparison_df = self.compare_all_models()

        # 6. ì‹œê°í™”
        self.visualize_predictions()
        self.visualize_metrics_comparison(comparison_df)

        # 7. ë¦¬í¬íŠ¸ ìƒì„±
        self.generate_report(comparison_df)

        print("\n" + "â–ˆ"*80)
        print("â–ˆ" + " "*28 + "ë¶„ì„ ì™„ë£Œ!" + " "*35 + "â–ˆ")
        print("â–ˆ"*80)
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("  - 01_ë°ì´í„°_ì‹œê°í™”.png")
        print("  - 02_ëª¨ë¸_ì˜ˆì¸¡_ë¹„êµ.png")
        print("  - 03_ì„±ëŠ¥_ì§€í‘œ_ë¹„êµ.png")
        print("  - 04_ë¶„ì„_ë¦¬í¬íŠ¸.txt")
        print("\n" + "â–ˆ"*80 + "\n")


# ============================================================================
# ë©”ì¸ ì‹¤í–‰
# ============================================================================

if __name__ == "__main__":
    # CSV íŒŒì¼ ê²½ë¡œ ì„¤ì •
    csv_file = "card_gyeonggi_202503 - ë³µì‚¬ë³¸.csv"

    # ë¶„ì„ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ë° ì‹¤í–‰
    predictor = ApplianceSalesPredictor(csv_file)
    predictor.run_full_analysis()
